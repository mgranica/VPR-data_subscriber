{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2b29e-9da2-496f-9175-8ef1103950a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import configparser\n",
    "import json\n",
    "\n",
    "import logging\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985b489-5c4c-4e60-b51d-54231863eb9e",
   "metadata": {},
   "source": [
    "# 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8d58d-57f5-42de-b007-ed8c8d39255d",
   "metadata": {},
   "source": [
    "## 1.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09b69f-ed05-4d6d-b0d3-9b88c34bcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "import logging\n",
    "# import numpy as np\n",
    "\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5ae74-14f5-4122-902e-d5bfa0c76d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spark_session import create_spark_session\n",
    "from schemas import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e496c-c675-4aff-b959-e01e2789c263",
   "metadata": {},
   "source": [
    "## 1.2 Extract AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d10977-0493-4574-aa51-14e3045895ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aws_credentials(profile_name=\"default\"):\n",
    "\n",
    "    # Load credentials from the .aws/credentials file (local development)\n",
    "    try:\n",
    "        credentials = configparser.ConfigParser()\n",
    "        credentials.read(os.path.join('..', '.aws', 'credentials'))\n",
    "        \n",
    "        logging.info(\"Successfully loaded credentials variables from .aws file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading .aws file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    aws_access_key_id = credentials[profile_name][\"aws_access_key_id\"]\n",
    "    aws_secret_access_key = credentials[profile_name][\"aws_secret_access_key\"]\n",
    "\n",
    "    if not aws_access_key_id or not aws_secret_access_key:\n",
    "        logging.error(\"AWS credentials not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return aws_access_key_id, aws_secret_access_key\n",
    "\n",
    "aws_access_key_id, aws_secret_access_key = load_aws_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6fad0-0b11-430a-9410-04ba5cc79c88",
   "metadata": {},
   "source": [
    "## 1.3 Extract AWS Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d14979-1932-406c-912e-351cb8b2fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aws_config():\n",
    "    \"\"\"\n",
    "    Loads AWS configuration settings from the .aws/config file.\n",
    "\n",
    "    :param profile_name: The profile name in the AWS config file (default: \"default\").\n",
    "    :return: The region_name as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(os.path.join('..', '.aws', 'config'))\n",
    "        logging.info(\"Successfully loaded config variables from .aws file.\")\n",
    "\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading .aws file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "config = load_aws_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62458-3216-4673-94c9-cd1619f33e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"vproptimiserplatform\"\n",
    "ORDERS = \"orders\"\n",
    "STREAM_NAME = \"orders_stream_3\"\n",
    "REGION = config[\"default\"][\"REGION\"]\n",
    "\n",
    "BRONZE = \"bronze\"\n",
    "SILVER = \"silver\"\n",
    "GOLD = \"gold\"\n",
    "DELTA = \"delta\"\n",
    "\n",
    "PROCESSING_TRIGGER = \"5 seconds\"\n",
    "\n",
    "## Append Checkpoints \n",
    "EVENTS_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/events\"\n",
    "ORDERS_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/orders\"\n",
    "ORDERS_ITEMS_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/orders_items\"\n",
    "\n",
    "## Update Checkpoints\n",
    "EVENTS_UPDATE_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/events_update\"\n",
    "ORDERS_UPDATE_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/orders_update\"\n",
    "ORDERS_ITEMS_UPDATE_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/orders_items_update\"\n",
    "PRODUCTS_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/inventory_update\"\n",
    "\n",
    "## paths\n",
    "EVENTS = \"events\"\n",
    "ORDERS = \"orders\"\n",
    "ORDERS_ITEMS = \"orders_items\"\n",
    "PRODUCTS = \"products_table\"\n",
    "\n",
    "# TODO TBC medallion architecture??\n",
    "EVENTS_PATH = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/{BRONZE}/{EVENTS}\"\n",
    "ORDERS_PATH = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/{BRONZE}/{ORDERS}\"\n",
    "ORDERS_ITEMS_PATH = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/{BRONZE}/{ORDERS_ITEMS}\"\n",
    "PRODUCTS_PATH = f\"s3a://{BUCKET_NAME}/{ORDERS}/{GOLD}/{PRODUCTS}\"\n",
    "\n",
    "ORDERS_STREAM_PATH = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/{BRONZE}/{STREAM_NAME}\"\n",
    "ORDERS_STREAM_CHECKPOINT_LOCATION = f\"s3a://{BUCKET_NAME}/{ORDERS}/{DELTA}/checkpoints/{STREAM_NAME}\"\n",
    "\n",
    "## Tables\n",
    "EVENTS_TABLE = f\"{EVENTS}_table\"\n",
    "ORDERS_TABLE = f\"{ORDERS}_table\"\n",
    "ORDERS_ITEMS_TABLE = f\"{ORDERS_ITEMS}_table\"\n",
    "\n",
    "TOPIC_NAME = \"order_stream\"\n",
    "BOOTSTRAP_SERVER = \"51.92.77.20:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e946633-22d0-427a-bb4d-897f9091d9b3",
   "metadata": {},
   "source": [
    "## 1.4 Initialise Spar Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51326936-4f99-4e91-9a4a-35f8b845ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Structured Streaming with Kafka Demo\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars\", \"file:///D://work//development//spark_structured_streaming_kafka//spark-sql-kafka-0-10_2.11-2.4.0.jar,\n",
    "        file:///D://work//development//spark_structured_streaming_kafka//kafka-clients-1.1.0.jar\") \\\n",
    "        .config(\"spark.executor.extraClassPath\", \"file:///D://work//development//spark_structured_streaming_kafka//spark-sql-kafka-0-10_2.11-2.4.0.jar:file:///D://work//development//spark_structured_streaming_kafka//kafka-clients-1.1.0.jar\") \\\n",
    "        .config(\"spark.executor.extraLibrary\", \"file:///D://work//development//spark_structured_streaming_kafka//spark-sql-kafka-0-10_2.11-2.4.0.jar:file:///D://work//development//spark_structured_streaming_kafka//kafka-clients-1.1.0.jar\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"file:///D://work//development//spark_structured_streaming_kafka//spark-sql-kafka-0-10_2.11-2.4.0.jar:file:///D://work//development//spark_structured_streaming_kafka//kafka-clients-1.1.0.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7758bb-619f-4aa2-be8c-9c707a964e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "def create_spark_session(aws_access_key_id, aws_secret_access_key, cores_number=\"2\"):\n",
    "    \"\"\"\n",
    "    Create and configure a Spark session with AWS credentials and required Kafka and Delta Lake packages.\n",
    "    \n",
    "    :param aws_access_key_id: AWS access key ID.\n",
    "    :param aws_secret_access_key: AWS secret access key.\n",
    "    :param cores_number: Number of cores to use for the Spark session (default is 2).\n",
    "    :return: SparkSession\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the Spark session with AWS, Kafka, and Delta Lake settings\n",
    "        conf = (\n",
    "            SparkConf()\n",
    "            .setAppName(\"VPR-data_landing\")\n",
    "            .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.eu-south-2.amazonaws.com\")\n",
    "            .set(\"spark.jars.packages\", \n",
    "                 \"io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.apache.kafka:kafka-clients:3.4.2\"\n",
    "                )\n",
    "            .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .set(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id)\n",
    "            .set(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key)\n",
    "            .setMaster(f\"local[{cores_number}]\")  # Use the specified number of cores\n",
    "        )\n",
    "        \n",
    "        # Build the Spark session with Delta configuration\n",
    "        builder = SparkSession.builder.config(conf=conf)\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "        return spark\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the Spark session: {str(e)}\")\n",
    "        raise  # Re-raise the exception after logging or handling\n",
    "\n",
    "spark = create_spark_session(aws_access_key_id, aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622ca40-01ac-496c-9407-7117c181b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_order_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVER)\n",
    "    .option(\"suscribe\", TOPIC_NAME)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44101792-93b6-4b4e-a644-af9eea2728e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a125bc-a3c9-4bae-aa53-ba1389830796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882447d-6729-4241-8809-5069439a24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis_order_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3369c5-ebce-4a48-a08d-d56838731749",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    kinesis_order_stream\n",
    "    .withColumn(\"json_data\", f.expr(\"CAST(unbase64(data) AS STRING)\"))\n",
    "    .select(\"json_data\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ce46a-9a43-48c6-8a48-cd0046252ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stream_test = ( \n",
    "#     kinesis_order_stream.writeStream\n",
    "#     .outputMode(\"append\")\n",
    "#     .queryName(\"kinesis_orders\")\n",
    "#     .option(\"checkpointLocation\", \"checkpoints/kineisis_orders_tests_13\")\n",
    "#     .option(\"kinesis.endpointUrl\", \"https://kinesis.eu-south-2.amazonaws.com\")\n",
    "#     .format(\"memory\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# spark.table(\"kinesis_orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa4936-e9bb-4375-8dd3-bb198d1cf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_stream = (\n",
    "    kinesis_order_stream\n",
    "    .withColumn(\"json_data\", f.expr(\"CAST(unbase64(data) AS STRING)\"))\n",
    "    .withColumn(\"orders\", f.from_json(\"json_data\", orders_schema))\n",
    "    .select(\"orders.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42574c9b-9339-45d4-8a35-d48ba5b5f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_stream\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265806b-466b-4361-a817-242c073ed196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d1c95-2210-4517-94c0-2692c9dc96f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940d330-fde6-4573-b8bd-598d39c83321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = boto3.client(\n",
    "#     'kinesis',\n",
    "#     aws_access_key_id=aws_access_key_id,\n",
    "#     aws_secret_access_key=aws_secret_access_key,\n",
    "#     region_name=REGION\n",
    "# )\n",
    "\n",
    "# response = client.describe_stream(StreamName=STREAM_NAME)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49e1be-78eb-488c-9219-c0ad033742d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = boto3.client(\n",
    "#     'kinesis',\n",
    "#     aws_access_key_id=aws_access_key_id,\n",
    "#     aws_secret_access_key=aws_secret_access_key,\n",
    "#     region_name=REGION\n",
    "# )\n",
    "\n",
    "# # Get the shard iterator for the stream\n",
    "# response = client.describe_stream(StreamName=STREAM_NAME)\n",
    "# shard_id = response['StreamDescription']['Shards'][0]['ShardId']\n",
    "\n",
    "# shard_iterator_response = client.get_shard_iterator(\n",
    "#     StreamName=STREAM_NAME,\n",
    "#     ShardId=shard_id,\n",
    "#     ShardIteratorType='TRIM_HORIZON'  # Use 'LATEST' for most recent records\n",
    "# )\n",
    "# shard_iterator = shard_iterator_response['ShardIterator']\n",
    "\n",
    "# # Fetch records\n",
    "# record_response = client.get_records(ShardIterator=shard_iterator, Limit=100)\n",
    "# record_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333763e-461c-4803-a0d0-838cc8f54e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f667e-ca5f-449d-816d-4b201a04d1c9",
   "metadata": {},
   "source": [
    "# 2. Orders Stream Suscriber workarround / Connection to kinesis failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad1093-c187-46f1-8cfe-836fea139586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kinesis_client(aws_access_key_id, aws_secret_access_key, region_name):\n",
    "    \"\"\"Initialize the Kinesis Boto3 client with error handling.\"\"\"\n",
    "    try:\n",
    "        kinesis_client = boto3.client(\n",
    "            'kinesis',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            region_name=region_name\n",
    "        )\n",
    "        print(\"Kinesis client initialized successfully.\")\n",
    "        return kinesis_client\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Kinesis client: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_kinesis_data(kinesis_client, stream_name, orders_schema, shard_iterator_type = \"TRIM_HORIZON\"):\n",
    "    \"\"\"\n",
    "    Fetch records from a Kinesis stream and convert them into a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - kinesis_client: Boto3 Kinesis client instance.\n",
    "    - stream_name: Name of the Kinesis stream to read from.\n",
    "    - orders_schema: Spark DataFrame schema to apply to the incoming records.\n",
    "    - shard_iterator_type: Use 'LATEST' for most recent records\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame containing the fetched records.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the shard iterator for the stream\n",
    "        response = kinesis_client.describe_stream(StreamName=stream_name)\n",
    "        shard_id = response['StreamDescription']['Shards'][0]['ShardId']\n",
    "\n",
    "        shard_iterator = kinesis_client.get_shard_iterator(\n",
    "            StreamName=stream_name,\n",
    "            ShardId=shard_id,\n",
    "            ShardIteratorType=shard_iterator_type\n",
    "        )['ShardIterator']\n",
    "\n",
    "        response = kinesis_client.get_records(ShardIterator=shard_iterator, Limit=500)\n",
    "        records = response['Records']\n",
    "\n",
    "        # Convert the Kinesis records into a list of JSON strings\n",
    "        records_data = [json.loads(record['Data']) for record in records]\n",
    "\n",
    "        # Create a Spark DataFrame from the fetched records\n",
    "        df = spark.createDataFrame(records_data, schema=orders_schema)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except kinesis_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Stream {stream_name} not found.\")\n",
    "    except kinesis_client.exceptions.ProvisionedThroughputExceededException:\n",
    "        print(\"Throughput limit exceeded, please try again later.\")\n",
    "    except kinesis_client.exceptions.InvalidArgumentException as e:\n",
    "        print(f\"Invalid argument: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data from Kinesis: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_df_as_delta(df, table_path, mode='append'):\n",
    "    \"\"\"\n",
    "    Saves a PySpark DataFrame as a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to be saved.\n",
    "    - table_path: The path for the Delta table.\n",
    "    - mode: The save mode for the table. Default is 'append'.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the saving process fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the DataFrame to a Delta table at the specified path\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(mode) \\\n",
    "            .save(table_path)\n",
    "        print(f\"DataFrame successfully saved as Delta table at: {table_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to save DataFrame as Delta table: {str(e)}\")\n",
    "\n",
    "def read_delta_table_as_stream(delta_table_path):\n",
    "    \"\"\"\n",
    "    Reads a Delta table as a streaming DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - delta_table_path: The path to the Delta table.\n",
    "\n",
    "    Returns:\n",
    "    - A streaming DataFrame representing the Delta table.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the streaming read process fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Delta table as a streaming DataFrame\n",
    "        streaming_df = (\n",
    "            spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .load(delta_table_path)\n",
    "        )\n",
    "        \n",
    "        return streaming_df  # Returning the streaming DataFrame\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to read Delta table as stream: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e5167-078c-4961-a440-8ca0537ae971",
   "metadata": {},
   "source": [
    "# 3. Write Orders Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a6463-3fc5-49f3-914e-6084fe5826fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb7527-a23f-4a88-b2b7-6a65b89fcacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_stream(df_order_stream, events_path, checkpoint_location):\n",
    "    \"\"\"\n",
    "    Processes the order stream by selecting event-specific columns and writing them \n",
    "    as a stream into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df_order_stream: The input PySpark DataFrame containing order stream data.\n",
    "    - events_path: The path where the Delta table for events should be written.\n",
    "    - checkpoint_location: The location where checkpoint data will be stored for fault-tolerance.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the streaming process fails or encounters an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the stream transformation and writing process\n",
    "        events_stream = (\n",
    "            df_order_stream\n",
    "            .select(\n",
    "                f.col(\"event_id\"),\n",
    "                f.col(\"event_type\"),\n",
    "                f.col(\"event_timestamp\"),\n",
    "                f.col(\"order_id\")\n",
    "            )\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(once=True)  # Using trigger(once=True) as per requirement\n",
    "            .option(\"path\", events_path)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .start()\n",
    "        )\n",
    "        \n",
    "        # Log the status of the streaming process\n",
    "        logging.info(f\"Streaming process status: {events_stream.status}\")\n",
    "        \n",
    "        print(f\"Events stream successfully written to {events_path}\")\n",
    "\n",
    "        # return stream\n",
    "        return events_stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log the error and raise an exception\n",
    "        logging.error(f\"Failed to process events stream: {str(e)}\")\n",
    "        raise Exception(f\"Failed to process events stream: {str(e)}\")\n",
    "\n",
    "def process_orders_stream(df_order_stream, orders_path, checkpoint_location):\n",
    "    \"\"\"\n",
    "    Processes the order stream by selecting specific columns related to orders and writing them \n",
    "    as a stream into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df_order_stream: The input PySpark DataFrame containing order stream data.\n",
    "    - orders_path: The path where the Delta table for orders should be written.\n",
    "    - checkpoint_location: The location where checkpoint data will be stored for fault-tolerance.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the streaming process fails or encounters an error.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        # Define the stream transformation and writing process\n",
    "        orders_stream = (\n",
    "            df_order_stream\n",
    "            .select(\n",
    "                f.col(\"order_id\"),\n",
    "                f.col(\"order_details.customer_id\").alias(\"customer_id\"),\n",
    "                f.col(\"order_details.total_weight\").alias(\"total_weight\"),\n",
    "                f.col(\"order_details.total_volume\").alias(\"total_volume\"),\n",
    "                f.col(\"order_details.total_amount\").alias(\"total_price\"),\n",
    "                f.col(\"order_details.order_timestamp\").alias(\"order_timestamp\"),\n",
    "                f.col(\"order_details.status\").alias(\"status\"),\n",
    "                f.col(\"order_details.destination_address.lat\").alias(\"lat\"),\n",
    "                f.col(\"order_details.destination_address.lon\").alias(\"lon\")\n",
    "            )\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(once=True)  # Using trigger(once=True) as per requirement\n",
    "            .option(\"path\", orders_path)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .start()\n",
    "        )\n",
    "        \n",
    "        # Log the status of the streaming process\n",
    "        logging.info(f\"Streaming process status: {orders_stream.status}\")\n",
    "        \n",
    "        print(f\"Orders stream successfully written to {orders_path}\")\n",
    "\n",
    "        # Return Stream\n",
    "        return orders_stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log the error and raise an exception\n",
    "        logging.error(f\"Failed to process orders stream: {str(e)}\")\n",
    "        raise Exception(f\"Failed to process orders stream: {str(e)}\")\n",
    "\n",
    "def process_orders_items_stream(df_order_stream: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the incoming order stream DataFrame by exploding order items,\n",
    "    generating inventory IDs, and adding a status column.\n",
    "\n",
    "    Parameters:\n",
    "    - df_order_stream (DataFrame): Input DataFrame representing the order stream.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Transformed streaming DataFrame with the required columns.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the input DataFrame is empty or has unexpected schema.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        # Validate input streaming DataFrame\n",
    "        # if df_order_stream is None or len(df_order_stream.columns) == 0:\n",
    "        #     raise ValueError(\"Input DataFrame is empty or not provided\")\n",
    "\n",
    "        # Apply the transformation logic for the streaming DataFrame\n",
    "        df_orders_items_stream = (\n",
    "            df_order_stream            \n",
    "            .withColumn(\"order_exploded\", f.explode(f.col(\"order_details.items\")))\n",
    "            .withColumn(\"package_exploded\", f.explode(f.col(\"order_exploded.packages\")))\n",
    "            .withColumn(\"inventory_id\", f.concat(f.lit(\"inv-\"), f.expr(\"uuid()\")))\n",
    "            .withColumn(\"items_quantity\", f.col(\"order_exploded.quantity\") * f.col(\"package_exploded.quantity\"))\n",
    "            .withColumn(\"items_weight\", f.col(\"items_quantity\") * f.col(\"package_exploded.weight\"))\n",
    "            .withColumn(\"items_volume\", f.col(\"items_quantity\") * f.col(\"package_exploded.volume\"))\n",
    "            .withColumn(\"status\", f.lit(\"PENDING\"))\n",
    "            .select(\n",
    "                f.col(\"inventory_id\"),\n",
    "                f.col(\"order_id\"),\n",
    "                f.col(\"order_exploded.product_id\").alias(\"product_id\"),\n",
    "                f.col(\"order_exploded.product_name\").alias(\"product_name\"),\n",
    "                f.col(\"order_exploded.price\").alias(\"order_price\"),\n",
    "                f.col(\"package_exploded.package_id\").alias(\"package_id\"),\n",
    "                f.col(\"package_exploded.subpackage_id\").alias(\"subpackage_id\"),\n",
    "                f.col(\"items_quantity\"),\n",
    "                f.col(\"items_weight\"),\n",
    "                f.col(\"items_volume\"),\n",
    "                f.col(\"order_details.order_timestamp\").alias(\"order_timestamp\"),\n",
    "                f.col(\"status\")\n",
    "            )\n",
    "        )\n",
    "        # Log success message\n",
    "        logging.info(\"Transformation applied successfully to the streaming DataFrame.\")\n",
    "        return df_orders_items_stream\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred during transformation: {str(e)}\")\n",
    "        raise  # Re-raise exception for further handling\n",
    "\n",
    "\n",
    "def write_orders_items_stream(df_orders_items_stream: DataFrame, path: str, checkpoint_location: str):\n",
    "    \"\"\"\n",
    "    Writes the transformed order items stream DataFrame to Delta format.\n",
    "\n",
    "    Parameters:\n",
    "    - df_orders_items_stream (DataFrame): Input streaming DataFrame representing the transformed order items stream.\n",
    "    - path (str): The destination path for the Delta table.\n",
    "    - checkpoint_location (str): The location for checkpointing the stream.\n",
    "\n",
    "    Returns:\n",
    "    - StreamingQuery: The StreamingQuery object representing the started stream.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the input DataFrame is empty or has unexpected schema.\n",
    "    - Exception: For any other errors during stream initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input streaming DataFrame\n",
    "        # if df_orders_items_stream is None or len(df_orders_items_stream.columns) == 0:\n",
    "        #     raise ValueError(\"Input DataFrame is empty or not provided\")\n",
    "\n",
    "        # Configure and start the streaming write\n",
    "        orders_items_stream = (\n",
    "            df_orders_items_stream\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(once=True)  # Use `once=True` for single run\n",
    "            .option(\"path\", path)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .start()\n",
    "        )\n",
    "\n",
    "        # Log success message\n",
    "        logging.info(\"Stream started successfully and data is being written to the Delta table.\")\n",
    "        return orders_items_stream\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred during stream write operation: {str(e)}\")\n",
    "        raise  # Re-raise exception for further handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98291e-ca03-4c0a-aa6f-b2cbe09160de",
   "metadata": {},
   "source": [
    "## 3.1 Process Events Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd24be-60f0-464c-a766-205726131419",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stream = process_events_stream(df_orders_stream, EVENTS_PATH, EVENTS_CHECKPOINT_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a927481-26e4-4a14-8ceb-759ce5820264",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stream.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138196d-d066-4965-8ecd-b3753270fc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e30f9-e0da-4935-afaf-2c451b154dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc3823-b01d-4454-8271-65b94518fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(EVENTS_PATH)\n",
    "    .orderBy(\"event_timestamp\", ascending=False)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce57b60-9f93-4e41-a998-3735ec2b44d0",
   "metadata": {},
   "source": [
    "## 3.2 Process Orders Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512672c7-3dd5-4a90-867f-519703ca28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_orders_stream(df_orders_stream, ORDERS_PATH, ORDERS_CHECKPOINT_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d141b1d-281a-47d2-896b-77fad601b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(ORDERS_PATH)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e74330-7831-4fb5-a930-f4ecd57b4b36",
   "metadata": {},
   "source": [
    "## 3.3 Process Orders Iventory Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6f0d3-b521-4cfc-9ab3-7390b9b1393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_items_stream = process_orders_items_stream(df_orders_stream)\n",
    "write_orders_items_stream(df_orders_items_stream, ORDERS_ITEMS_PATH, ORDERS_ITEMS_CHECKPOINT_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac567b-df5b-485f-9be0-9df26f121a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(ORDERS_ITEMS_PATH)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08c317-4687-4eb9-95a3-af1be36fa59c",
   "metadata": {},
   "source": [
    "# 4. Write Inventory Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b1312-ef6f-40c3-8b35-e02415e78db0",
   "metadata": {},
   "source": [
    "## 4.1 Upsert Orders Items Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8e940-9000-4858-b3eb-7f2d8618f5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_packages = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(f\"s3a://{BUCKET_NAME}/{ORDERS}/{GOLD}/package_table\")\n",
    ")\n",
    "df_packages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3680-8a38-4424-a480-1ae4768745d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed5c46-4c34-4e0f-9eb9-33bf971e616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d3373-ecd5-40ae-a620-5f33a097921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_packages.dropDuplicates([\"package_id\", \"subpackage_id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38eb2a-eea6-432f-ba05-74eb36fd471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_package(microBatchDF, batchId):\n",
    "    \"\"\"\n",
    "    Upserts the incoming micro-batch DataFrame into the Delta table for products.\n",
    "\n",
    "    Parameters:\n",
    "    - microBatchDF: The micro-batch DataFrame from the streaming source.\n",
    "    - batchId: The unique identifier for the micro-batch.\n",
    "    \"\"\"\n",
    "    deltaTableProducts = DeltaTable.forPath(spark, f\"s3a://{BUCKET_NAME}/{ORDERS}/{GOLD}/package_table\") #PRODUCTS_PATH)\n",
    "\n",
    "    # WorkArround Preprocess the micro-batch DataFrame to removemicrobath duplicates integrity lo\n",
    "    deduplicatedBatchDF = (\n",
    "        microBatchDF\n",
    "        .groupBy(\"package_id\", \"subpackage_id\")\n",
    "        .agg(\n",
    "            f.sum(\"items_quantity\").alias(\"items_quantity\"),\n",
    "        )\n",
    "    )\n",
    "    (\n",
    "        deltaTableProducts.alias(\"t\")\n",
    "        .merge(\n",
    "            deduplicatedBatchDF.alias(\"s\"),\n",
    "            \"s.package_id = t.package_id AND s.subpackage_id = t.subpackage_id\"  \n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            # Quantity availability to fulfill order\n",
    "            condition=f.col(\"t.stock_quantity\") >= f.col(\"s.items_quantity\"),\n",
    "            set={\n",
    "                \"stock_quantity\": f.col(\"t.stock_quantity\") - f.col(\"s.items_quantity\"),\n",
    "                # \"updated_at\": f.current_timestamp()\n",
    "            },\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "def update_orders_items_stream(\n",
    "    df_orders_items_stream: DataFrame, \n",
    "    packages_path: str, \n",
    "    checkpoint_location: str, \n",
    "    upsert_function\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes and starts a streaming query to update the products table using the upsert function.\n",
    "\n",
    "    Parameters:\n",
    "    - df_orders_items_stream (DataFrame): The input streaming DataFrame containing orders items data.\n",
    "    - packages_path (str): The path to the Delta table where the packages data will be stored.\n",
    "    - checkpoint_location (str): The checkpoint location for the stream query.\n",
    "    - upsert_function (function): The function to perform upsert operations on the Delta table.\n",
    "\n",
    "    Returns:\n",
    "    - StreamingQuery: The StreamingQuery object representing the started stream.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Start the streaming write process with upsert logic using foreachBatch\n",
    "        update_products_stream = (\n",
    "            df_orders_items_stream\n",
    "            .select(\n",
    "                f.col(\"package_id\"),\n",
    "                f.col(\"subpackage_id\"),\n",
    "                f.col(\"items_quantity\")\n",
    "            )\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"update\")\n",
    "            .foreachBatch(upsert_to_package)  # Use the provided upsert function\n",
    "            .option(\"path\", packages_path)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .trigger(once=True)  # Single-trigger mode for processing the micro-batch once\n",
    "            .start()\n",
    "        )\n",
    "\n",
    "        # Log successful stream start\n",
    "        logging.info(\"Streaming query for packages update started successfully.\")\n",
    "        return update_products_stream\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting the streaming query for packagess update: {str(e)}\")\n",
    "        raise  # Re-raise exception for further handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716efdf-9aa9-4b15-a741-f8fbdbae2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update_orders_items_stream = update_orders_items_stream(\n",
    "    df_orders_items_stream, \n",
    "    f\"s3a://{BUCKET_NAME}/{ORDERS}/{GOLD}/package_table\", \n",
    "    ORDERS_ITEMS_UPDATE_CHECKPOINT_LOCATION, \n",
    "    upsert_to_package\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cd91c-6b04-4fc3-bc1c-3d2949c63ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(f\"s3a://{BUCKET_NAME}/{ORDERS}/{GOLD}/package_table\")\n",
    "    .orderBy([\"stock_quantity\"], ascending=False)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d86bb-bcb8-482f-9bd0-af71c73fde7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
